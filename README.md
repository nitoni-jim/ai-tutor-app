ðŸš€ AI Tutor App â€” Intelligent Tutoring System for WAEC / NECO / JAMB

A research-driven project applying Machine Learning, NLP, and RAG for automated exam preparation.








ðŸŒŸ Overview

AI Tutor App is an intelligent tutoring system designed to support candidates preparing for WAEC, NECO, and JAMB examinations in Nigeria.
This project integrates:

Natural Language Processing (NLP)

Machine Learning (ML)

Semantic Search using Embeddings

Retrieval-Augmented Generation (RAG)

The system performs:

Automated question cleaning & normalization

Semantic embedding of questions

Vector-based retrieval of similar items

LLM-style explanation generation (placeholder for now)

Clear modular structure for future deep learning extension

This repository accompanies my graduate applications to UCSD and Arizona State University (ASU) and demonstrates applied AI engineering and research readiness.

ðŸ“‚ Repository Structure
ai-tutor-app/
â”‚
â”œâ”€â”€ preprocessing/          # Text cleaning & math normalization modules
â”‚   â”œâ”€â”€ text_cleaning.py
â”‚   â””â”€â”€ math_cleaning.py
â”‚
â”œâ”€â”€ embeddings/             # Embedding creation & vector store
â”‚   â”œâ”€â”€ generate_embeddings.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚
â”œâ”€â”€ rag/                    # Retrieval and generation pipeline
â”‚   â”œâ”€â”€ retriever.py
â”‚   â”œâ”€â”€ generator.py
â”‚   â””â”€â”€ pipeline.py         # (future full RAG pipeline)
â”‚
â”œâ”€â”€ evaluation/             # Metrics for retrieval/classification
â”‚   â””â”€â”€ metrics.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ experiments_full.ipynb   # Full cleaning â†’ embedding â†’ retrieval â†’ generation notebook
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ metadata/           # Saved embeddings, vector store, lookup files
â”‚
â”œâ”€â”€ run_demo.py             # Minimal CLI demo pipeline
â””â”€â”€ requirements.txt        # Dependencies

## ðŸ§© System Architecture

```mermaid
flowchart TD
    A[Raw Exam Questions<br>WAEC / NECO / JAMB] --> B[Preprocessing<br>Text + Math Cleaning]
    B --> C[Embeddings<br>MiniLM / Fallback]
    C --> D[Vector Store<br>.npy + JSON]
    D --> E[Retriever<br>Cosine Similarity]
    E --> F[Generator<br>Placeholder RAG]
    F --> G[Explanations<br>Future LLM Integration]

ðŸ§  Core Features
ðŸ”¹ 1. Question Cleaning & Normalization

Handles:

Unicode inconsistencies

Excess whitespace

Mathematical symbol normalization (Ã— â†’ *, Ã· â†’ /)

Removal of control characters

This ensures consistent text before embedding.

ðŸ”¹ 2. Embedding Generation

Uses:

sentence-transformers/all-MiniLM-L6-v2 (if available)

Otherwise a deterministic fallback for reproducibility

Embeddings are stored in:

data/metadata/vector_store.npy
data/metadata/vector_store_texts.json

ðŸ”¹ 3. Semantic Retrieval

Implements cosine-similarity retrieval:

retriever.retrieve(query_vector, top_k=3)


Used to find semantically similar exam questions.

ðŸ”¹ 4. Placeholder RAG Generation

Combines retrieved context into a structured template.

In the future this will be replaced by an actual LLM API integration (OpenAI, HuggingFace, Phi, etc.)

ðŸ”¹ 5. Evaluation Tools

Includes:

Recall@k

MRR (Mean Reciprocal Rank)

Basic classification metrics

All in evaluation/metrics.py.

ðŸ§ª Experiments Notebook

The notebook:

notebooks/experiments_full.ipynb


Contains:

Data cleaning steps

Embedding generation

Vector indexing

Retrieval demonstration

Explanation generation

Evaluation examples

This notebook is designed for academic review and ML reproducibility.

ðŸ“˜ Research Questions

How can we best normalize mixed-format questions (math + text) for NLP models?

Can embeddings detect curriculum-equivalent WAEC/NECO/JAMB items?

How effective is a RAG pipeline in explaining exam questions?

Can lightweight models help under-resourced students learn more effectively?

ðŸ§­ Future Roadmap
ðŸ”œ Phase 1: Data Expansion

Bulk ingestion of WAEC/NECO past questions

Automated difficulty labeling

Topic tag prediction (syllabus mapping)

ðŸ”œ Phase 2: Model Improvement

FAISS vector index

Better embedding models (e.g., bge-large)

Classification model for question topics

ðŸ”œ Phase 3: Full RAG System

Fine-tuned model for exam solutions

Multi-step reasoning

Structured explanations (steps, diagrams)

ðŸ”œ Phase 4: Mobile App Integration

Android app (Java / Kotlin)

Student performance analytics

Offline-first learning

ðŸ§¾ Citation

If referencing this work:

@misc{jimogbolo2025aitutor,
  title={AI-Tutor-App: An Intelligent Tutoring System for WAEC/NECO/JAMB Exams},
  author={Nitoni Jim-Ogbolo},
  year={2025},
  url={https://github.com/nitoni-jim/ai-tutor-app}
}

ðŸ“¬ Contact

Nitoni Jim-Ogbolo
AI Developer & Research Enthusiast
Email: nitoni4fj@gmail.com
